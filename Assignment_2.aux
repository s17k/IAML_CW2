\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}: (30 total points) Image data analysis with PCA}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}(3 points) Once you have applied the normalisation from Step 1 to Step 4 above, report the values of the first 4 elements for the first training sample in \texttt  {Xtrn\_nm}, i.e. \texttt  {Xtrn\_nm[0,:]} and the last training sample, i.e. \texttt  {Xtrn\_nm[-1,:]}. }{1}{subsection.1.1}}
\newlabel{Q1.1}{{1.1}{1}{(3 points) Once you have applied the normalisation from Step 1 to Step 4 above, report the values of the first 4 elements for the first training sample in \texttt {Xtrn\_nm}, i.e. \texttt {Xtrn\_nm[0,:]} and the last training sample, i.e. \texttt {Xtrn\_nm[-1,:]}. }{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}(4 points) Using {\tt  Xtrn} and Euclidean distance measure, for each class, find the two closest samples and two furthest samples of that class to the mean vector of the class. }{2}{subsection.1.2}}
\newlabel{Q1.2}{{1.2}{2}{(4 points) Using {\tt Xtrn} and Euclidean distance measure, for each class, find the two closest samples and two furthest samples of that class to the mean vector of the class. }{subsection.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}(3 points) Apply Principal Component Analysis (PCA) to the data of {\tt  Xtrn\_nm} using \href  {https://scikit-learn.org/0.19/modules/generated/sklearn.decomposition.PCA.html}{sklearn.decomposition.PCA}, and report the variances of projected data for the first five principal components in a table. Note that you should use {\tt  Xtrn\_nm} instead of {\tt  Xtrn}. }{3}{subsection.1.3}}
\newlabel{Q1.pca.variance}{{1.3}{3}{(3 points) Apply Principal Component Analysis (PCA) to the data of {\tt Xtrn\_nm} using \href {https://scikit-learn.org/0.19/modules/generated/sklearn.decomposition.PCA.html}{sklearn.decomposition.PCA}, and report the variances of projected data for the first five principal components in a table. Note that you should use {\tt Xtrn\_nm} instead of {\tt Xtrn}. }{subsection.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}(3 points) Plot a graph of the cumulative explained variance ratio as a function of the number of principal components, $K$, where $1 \le K \le 784$. Discuss the result briefly. }{4}{subsection.1.4}}
\newlabel{Q1.plot.pca.variance}{{1.4}{4}{(3 points) Plot a graph of the cumulative explained variance ratio as a function of the number of principal components, $K$, where $1 \le K \le 784$. Discuss the result briefly. }{subsection.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}(4 points) Display the images of the first 10 principal components in a 2-by-5 grid, putting the image of 1st principal component on the top left corner, followed by the one of 2nd component to the right. Discuss your findings briefly. }{5}{subsection.1.5}}
\newlabel{Q1.disp.pca}{{1.5}{5}{(4 points) Display the images of the first 10 principal components in a 2-by-5 grid, putting the image of 1st principal component on the top left corner, followed by the one of 2nd component to the right. Discuss your findings briefly. }{subsection.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}(5 points) Using \texttt  {Xtrn\_nm}, for each class and for each number of principal components $K = 5, 20, 50, 200$, apply dimensionality reduction with PCA to the first sample in the class, reconstruct the sample from the dimensionality-reduced sample, and report the Root Mean Square Error (RMSE) between the original sample in {\tt  Xtrn\_nm} and reconstructed one. }{6}{subsection.1.6}}
\newlabel{Q1.6}{{1.6}{6}{(5 points) Using \texttt {Xtrn\_nm}, for each class and for each number of principal components $K = 5, 20, 50, 200$, apply dimensionality reduction with PCA to the first sample in the class, reconstruct the sample from the dimensionality-reduced sample, and report the Root Mean Square Error (RMSE) between the original sample in {\tt Xtrn\_nm} and reconstructed one. }{subsection.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}(4 points) Display the image for each of the reconstructed samples in a 10-by-4 grid, where each row corresponds to a class and each row column corresponds to a value of $K=5, \tmspace  +\thickmuskip {.2777em} 20, \tmspace  +\thickmuskip {.2777em} 50, \tmspace  +\thickmuskip {.2777em} 200$. }{7}{subsection.1.7}}
\newlabel{Q1.7}{{1.7}{7}{(4 points) Display the image for each of the reconstructed samples in a 10-by-4 grid, where each row corresponds to a class and each row column corresponds to a value of $K=5, \; 20, \; 50, \; 200$. }{subsection.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}(4 points) Plot all the training samples (\texttt  {Xtrn\_nm}) on the two-dimensional PCA plane you obtained in Question\nobreakspace  {}\ref  {Q1.pca.variance}, where each sample is represented as a small point with a colour specific to the class of the sample. Use the 'coolwarm' colormap for plotting. }{8}{subsection.1.8}}
\newlabel{Q1.8}{{1.8}{8}{(4 points) Plot all the training samples (\texttt {Xtrn\_nm}) on the two-dimensional PCA plane you obtained in \refQ {Q1.pca.variance}, where each sample is represented as a small point with a colour specific to the class of the sample. Use the 'coolwarm' colormap for plotting. }{subsection.1.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}: (25 total points) Logistic regression and SVM}{9}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}(3 points) Carry out a classification experiment with \href  {https://scikit-learn.org/0.19/modules/generated/sklearn.linear\_model.LogisticRegression.html}{multinomial logistic regression}, and report the classification accuracy and confusion matrix (in numbers rather than in graphical representation such as heatmap) for the test set. }{9}{subsection.2.1}}
\newlabel{Q2.1}{{2.1}{9}{(3 points) Carry out a classification experiment with \href {https://scikit-learn.org/0.19/modules/generated/sklearn.linear\_model.LogisticRegression.html}{multinomial logistic regression}, and report the classification accuracy and confusion matrix (in numbers rather than in graphical representation such as heatmap) for the test set. }{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}(3 points) Carry out a classification experiment with \href  {https://scikit-learn.org/0.19/modules/generated/sklearn.svm.SVC.html}{SVM classifiers}, and report the mean accuracy and confusion matrix (in numbers) for the test set. }{10}{subsection.2.2}}
\newlabel{Q2.2}{{2.2}{10}{(3 points) Carry out a classification experiment with \href {https://scikit-learn.org/0.19/modules/generated/sklearn.svm.SVC.html}{SVM classifiers}, and report the mean accuracy and confusion matrix (in numbers) for the test set. }{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}(6 points) We now want to visualise the decision regions for the logistic regression classifier we trained in Question\nobreakspace  {}\ref  {Q2.1}. }{11}{subsection.2.3}}
\newlabel{Q2.3}{{2.3}{11}{(6 points) We now want to visualise the decision regions for the logistic regression classifier we trained in \refQ {Q2.1}. }{subsection.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}(4 points) Using the same method as the one above, plot the decision regions for the SVM classifier you trained in Question\nobreakspace  {}\ref  {Q2.2}. Comparing the result with that you obtained in Question\nobreakspace  {}\ref  {Q2.3}, discuss your findings briefly. }{12}{subsection.2.4}}
\newlabel{Q2.4}{{2.4}{12}{(4 points) Using the same method as the one above, plot the decision regions for the SVM classifier you trained in \refQ {Q2.2}. Comparing the result with that you obtained in \refQ {Q2.3}, discuss your findings briefly. }{subsection.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}(6 points) We used default parameters for the SVM in Question\nobreakspace  {}\ref  {Q2.2}. We now want to tune the parameters by using cross-validation. To reduce the time for experiments, you pick up the first 1000 training samples from each class to create \texttt  {Xsmall}, so that \texttt  {Xsmall} contains 10,000 samples in total. Accordingly, you create labels, \texttt  {Ysmall}. }{13}{subsection.2.5}}
\newlabel{Q2.5}{{2.5}{13}{(6 points) We used default parameters for the SVM in \refQ {Q2.2}. We now want to tune the parameters by using cross-validation. To reduce the time for experiments, you pick up the first 1000 training samples from each class to create \texttt {Xsmall}, so that \texttt {Xsmall} contains 10,000 samples in total. Accordingly, you create labels, \texttt {Ysmall}. }{subsection.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}(3 points) Train the SVM classifier on the whole training set by using the optimal value of $C$ you found in Question\nobreakspace  {}\ref  {Q2.5}. }{14}{subsection.2.6}}
\newlabel{Q2.6}{{2.6}{14}{(3 points) Train the SVM classifier on the whole training set by using the optimal value of $C$ you found in \refQ {Q2.5}. }{subsection.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}: (20 total points) Clustering and Gaussian Mixture Models}{15}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}(3 points) Apply k-means clustering on {\tt  Xtrn} for $k = 22$, where we use \href  {https://scikit-learn.org/0.19/modules/generated/sklearn.cluster.KMeans.html}{sklearn.cluster.KMeans} with the parameters {\tt  n\_clusters=22} and {\tt  random\_state=1}. Report the sum of squared distances of samples to their closest cluster centre, and the number of samples for each cluster. }{15}{subsection.3.1}}
\newlabel{Q3.1}{{3.1}{15}{(3 points) Apply k-means clustering on {\tt Xtrn} for $k = 22$, where we use \href {https://scikit-learn.org/0.19/modules/generated/sklearn.cluster.KMeans.html}{sklearn.cluster.KMeans} with the parameters {\tt n\_clusters=22} and {\tt random\_state=1}. Report the sum of squared distances of samples to their closest cluster centre, and the number of samples for each cluster. }{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}(3 points) Using the training set only, calculate the mean vector for each language, and plot the mean vectors of all the 22 languages on a 2D-PCA plane, where you apply PCA on the set of 22 mean vectors without applying standardisation. On the same figure, plot the cluster centres obtained in Question\nobreakspace  {}\ref  {Q3.1}. }{16}{subsection.3.2}}
\newlabel{Q3.2}{{3.2}{16}{(3 points) Using the training set only, calculate the mean vector for each language, and plot the mean vectors of all the 22 languages on a 2D-PCA plane, where you apply PCA on the set of 22 mean vectors without applying standardisation. On the same figure, plot the cluster centres obtained in \refQ {Q3.1}. }{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}(3 points) We now apply hierarchical clustering on the training data set to see if there are any structures in the spoken languages. }{17}{subsection.3.3}}
\newlabel{Q3.3}{{3.3}{17}{(3 points) We now apply hierarchical clustering on the training data set to see if there are any structures in the spoken languages. }{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}(5 points) We here extend the hierarchical clustering done in Question\nobreakspace  {}\ref  {Q3.3} by using multiple samples from each language. }{18}{subsection.3.4}}
\newlabel{Q3.4}{{3.4}{18}{(5 points) We here extend the hierarchical clustering done in \refQ {Q3.3} by using multiple samples from each language. }{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}(6 points) We now consider Gaussian mixture model (GMM), whose probability distribution function (pdf) is given as a linear combination of Gaussian or normal distributions, i.e., }{19}{subsection.3.5}}
\newlabel{Q3.5}{{3.5}{19}{(6 points) We now consider Gaussian mixture model (GMM), whose probability distribution function (pdf) is given as a linear combination of Gaussian or normal distributions, i.e., }{subsection.3.5}{}}
\newlabel{LastPage}{{}{19}{}{page.19}{}}
\xdef\lastpage@lastpage{19}
\xdef\lastpage@lastpageHy{19}
