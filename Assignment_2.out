\BOOKMARK [1][-]{section.1}{: \(30 total points\) Image data analysis with PCA}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{\(3 points\) Once you have applied the normalisation from Step 1 to Step 4 above, report the values of the first 4 elements for the first training sample in Xtrn\137nm, i.e. Xtrn\137nm[0,:] and the last training sample, i.e. Xtrn\137nm[-1,:]. }{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{\(4 points\) Using Xtrn and Euclidean distance measure, for each class, find the two closest samples and two furthest samples of that class to the mean vector of the class. }{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{\(3 points\) Apply Principal Component Analysis \(PCA\) to the data of Xtrn\137nm using sklearn.decomposition.PCA, and report the variances of projected data for the first five principal components in a table. Note that you should use Xtrn\137nm instead of Xtrn. }{section.1}% 4
\BOOKMARK [2][-]{subsection.1.4}{\(3 points\) Plot a graph of the cumulative explained variance ratio as a function of the number of principal components, K, where 1 K 784. Discuss the result briefly. }{section.1}% 5
\BOOKMARK [2][-]{subsection.1.5}{\(4 points\) Display the images of the first 10 principal components in a 2-by-5 grid, putting the image of 1st principal component on the top left corner, followed by the one of 2nd component to the right. Discuss your findings briefly. }{section.1}% 6
\BOOKMARK [2][-]{subsection.1.6}{\(5 points\) Using Xtrn\137nm, for each class and for each number of principal components K = 5, 20, 50, 200, apply dimensionality reduction with PCA to the first sample in the class, reconstruct the sample from the dimensionality-reduced sample, and report the Root Mean Square Error \(RMSE\) between the original sample in Xtrn\137nm and reconstructed one. }{section.1}% 7
\BOOKMARK [2][-]{subsection.1.7}{\(4 points\) Display the image for each of the reconstructed samples in a 10-by-4 grid, where each row corresponds to a class and each row column corresponds to a value of K=5, \04020, \04050, \040200. }{section.1}% 8
\BOOKMARK [2][-]{subsection.1.8}{\(4 points\) Plot all the training samples \(Xtrn\137nm\) on the two-dimensional PCA plane you obtained in Question 1.3, where each sample is represented as a small point with a colour specific to the class of the sample. Use the 'coolwarm' colormap for plotting. }{section.1}% 9
\BOOKMARK [1][-]{section.2}{: \(25 total points\) Logistic regression and SVM}{}% 10
\BOOKMARK [2][-]{subsection.2.1}{\(3 points\) Carry out a classification experiment with multinomial logistic regression, and report the classification accuracy and confusion matrix \(in numbers rather than in graphical representation such as heatmap\) for the test set. }{section.2}% 11
\BOOKMARK [2][-]{subsection.2.2}{\(3 points\) Carry out a classification experiment with SVM classifiers, and report the mean accuracy and confusion matrix \(in numbers\) for the test set. }{section.2}% 12
\BOOKMARK [2][-]{subsection.2.3}{\(6 points\) We now want to visualise the decision regions for the logistic regression classifier we trained in Question 2.1. }{section.2}% 13
\BOOKMARK [2][-]{subsection.2.4}{\(4 points\) Using the same method as the one above, plot the decision regions for the SVM classifier you trained in Question 2.2. Comparing the result with that you obtained in Question 2.3, discuss your findings briefly. }{section.2}% 14
\BOOKMARK [2][-]{subsection.2.5}{\(6 points\) We used default parameters for the SVM in Question 2.2. We now want to tune the parameters by using cross-validation. To reduce the time for experiments, you pick up the first 1000 training samples from each class to create Xsmall, so that Xsmall contains 10,000 samples in total. Accordingly, you create labels, Ysmall. }{section.2}% 15
\BOOKMARK [2][-]{subsection.2.6}{\(3 points\) Train the SVM classifier on the whole training set by using the optimal value of C you found in Question 2.5. }{section.2}% 16
\BOOKMARK [1][-]{section.3}{: \(20 total points\) Clustering and Gaussian Mixture Models}{}% 17
\BOOKMARK [2][-]{subsection.3.1}{\(3 points\) Apply k-means clustering on Xtrn for k = 22, where we use sklearn.cluster.KMeans with the parameters n\137clusters=22 and random\137state=1. Report the sum of squared distances of samples to their closest cluster centre, and the number of samples for each cluster. }{section.3}% 18
\BOOKMARK [2][-]{subsection.3.2}{\(3 points\) Using the training set only, calculate the mean vector for each language, and plot the mean vectors of all the 22 languages on a 2D-PCA plane, where you apply PCA on the set of 22 mean vectors without applying standardisation. On the same figure, plot the cluster centres obtained in Question 3.1. }{section.3}% 19
\BOOKMARK [2][-]{subsection.3.3}{\(3 points\) We now apply hierarchical clustering on the training data set to see if there are any structures in the spoken languages. }{section.3}% 20
\BOOKMARK [2][-]{subsection.3.4}{\(5 points\) We here extend the hierarchical clustering done in Question 3.3 by using multiple samples from each language. }{section.3}% 21
\BOOKMARK [2][-]{subsection.3.5}{\(6 points\) We now consider Gaussian mixture model \(GMM\), whose probability distribution function \(pdf\) is given as a linear combination of Gaussian or normal distributions, i.e., }{section.3}% 22
